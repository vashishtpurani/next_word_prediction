# -*- coding: utf-8 -*-
"""next_word_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lF1V7_qMNP3eGXxa0zmUyDn9XpHp3y8B
"""

import random
import pickle
import pandas as pd
import numpy as np
from nltk.tokenize import RegexpTokenizer

from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Activation
from tensorflow.keras.optimizers import RMSprop

text_df = pd.read_fwf("input.txt")

text_df

with open("input.txt", "r") as file:
    text = file.readlines()
print(text)

"""text"""

text = [line.strip() for line in text if line.strip() and len(line.split()) > 1]

text

text = list (text)

text

joined_text = " ".join(text)

joined_text

#partial_split = joined_text[:10000]
partial_split = joined_text

tokenizer = RegexpTokenizer(r"\w+")
tokens = tokenizer.tokenize(partial_split.lower())

tokens

unique_tokens = np.unique(tokens)

unique_token_index = {token: idx for idx, token in enumerate(unique_tokens)}

unique_token_index

number_words = 10
input_words = []
next_words = []

for i in range(len(tokens)-number_words):
  input_words.append(tokens[i:i + number_words])
  next_words.append(tokens[i + number_words])

input_words

next_words

X = np.zeros((len(input_words),number_words,len(unique_tokens)), dtype = bool)

Y = np.zeros((len(next_words),len(unique_tokens)), dtype = bool)

X

Y

for i,words in enumerate(input_words):
  for j,word in enumerate(words):
    X[i,j,unique_token_index[word]] = 1
  Y[i,unique_token_index[next_words[i]]]=1

X,Y

model = Sequential()
model.add(LSTM(128,input_shape=(number_words,len(unique_tokens)),return_sequences=True))
model.add(LSTM(128))
model.add(Dense(len(unique_tokens)))
model.add(Activation("softmax"))

model.compile(loss="categorical_crossentropy",optimizer=RMSprop(learning_rate=0.01),metrics=["accuracy"])
model.fit(X,Y,batch_size=128,epochs=30,shuffle=True)

model.save("second_all_data.h5")

model = load_model("second_all_data.h5")

def predict_word(input_text,best):
  input_text = input_text.lower()
  X = np.zeros((1,number_words,len(unique_tokens)))
  for i,word in enumerate(input_text.split()):
    X[0,i,unique_token_index[word]]=1

  predictions = model.predict(X)[0]
  return np.argpartition(predictions, -best)[-best:]

sen = predict_word("You are all resolved rather to die than to",5)

print([unique_tokens[inx] for inx in sen])

def txt_gen(input_text,txt_len,creativity=3):
  word_sequence = input_text.split()
  current = 0
  for _ in range(txt_len):
    sub_sequence = "" .join(tokenizer.tokenize("".join(word_sequence).lower())[current:current+number_words])
    try:
      choice = unique_tokens[random.choice(predict_word(sub_sequence,creativity))]
    except:
      choice = random.choice(unique_tokens)
    word_sequence.append(choice)
    current+=1
  return " ".join(word_sequence)

txt_gen("You are all resolved rather to die than to",100,10)

